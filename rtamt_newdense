#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Mar 19 15:03:00 2023

@author: lay0005
"""

import numpy as np
import random
import matplotlib.pyplot as plt
import matplotlib
import rtamt

grid_size = 6

def specification(spec_str):
    spec = rtamt.StlDenseTimeSpecification()
    spec.name = 'STL discrete-time online Python monitor'
    spec.declare_var('x', 'float')
    spec.declare_var('y', 'float')
    spec.spec = spec_str
    spec.parse()
    return spec

def robustness(dataset, spec):
    rob = spec.evaluate(['x',dataset[0]],['y',dataset[1]])
    return [i[1] for i in rob]

# stl initialize
spec_goal = specification('x>=4 & y>=4') #specification representing the goal location

spec_obs = specification('(x>=2 & x < 5 & y >= 2 & y < 3) or (x > 3 & x <= 4 & y > 2 & y <= 3)') #specification representing the obstacle location


spec = specification('(x>=4 & y>=4) and ((x>=2 & x < 5 & y >= 2 & y < 3) or (x > 3 & x <= 4 & y > 2 & y <= 3))')
    
def state_transition(current_state, action):
    if action == (0,-1):#down
        suggested_state = (current_state[0] + action[0], max(0,current_state[1] + action[1]))
    elif action == (0,1):#up
        suggested_state = (current_state[0] + action[0], min(5,current_state[1] + action[1]))
    elif action ==(-1,0):#left
        suggested_state = (max(0,current_state[0] + action[0]), current_state[1] + action[1])
    elif action == (1,0):#right
        suggested_state = (min(5,current_state[0] + action[0]), current_state[1] + action[1])
#    
#    if action == (0,-1):#down
#        if (current_state[1] + action[1])<0:
#            suggested_state = (current_state[0] + action[0],current_state[1] + -action[1])
#        else:
#            suggested_state = (current_state[0] + action[0],current_state[1] + action[1])
#    elif action == (0,1):#up
#        if (current_state[1] + action[1])>5:
#            suggested_state = (current_state[0] + action[0],current_state[1] + -action[1])
#        else:
#            suggested_state = (current_state[0] + action[0],current_state[1] + action[1])
#    elif action ==(-1,0):#left
#        if (current_state[0] + action[0])<0:
#            suggested_state = (current_state[0] - action[0],current_state[1] + action[1])
#        else:
#            suggested_state = (current_state[0] + action[0],current_state[1] + action[1])
#    elif action == (1,0):#right
#        if (current_state[0] + action[0])>5:
#            suggested_state = (current_state[0] + -action[0],current_state[1] + action[1])
#        else:
#            suggested_state = (current_state[0] + action[0],current_state[1] + action[1])
#    
    # up, down,right, left
    states = [(current_state[0] + 0, min(5,current_state[1] + 1)), (current_state[0] + 0, max(0,current_state[1]  - 1)), ((min(5,current_state[0] + 1), current_state[1] + 0)) ,(max(0,current_state[0] -1), current_state[1] + 0)]
   
    
    num_states = len(states)
    transition_probabilities = [0.8 if state == suggested_state else (0.2/(num_states-1)) for state in states]
    next_state = random.choices(states,weights=transition_probabilities) 
    return next_state[0]


# Define the actions
actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # up, down,right, left

# Initialize Q-table
Q = np.zeros((grid_size, grid_size, len(actions)))

# Define the training parameters
alpha = 0.1
gamma = 0.9
epsilon = 0.9
max_steps = 20
num_episodes = 10000
decay_rate = -0.005
er = []
# Train the agent
for episode in range(num_episodes):
    print(episode)
    sts = []
    # Initialize the state
    current_state = (0,0)
    epsilon = np.exp(-0.005*episode)
    sts.append(current_state)
    # Terminate the episode if the agent reaches the goal or the maximum number of steps is reached
    for step in range(max_steps):
        # Choose an action using epsilon-greedy exploration
        if random.uniform(0, 1) < epsilon: #explore
            action = random.choice(actions)
        else:
            action = actions[np.argmax(Q[current_state[0], current_state[1]])] #exploit
                              
        # Determine the next state following the transition probablities        
        next_state = state_transition(current_state,action)
        
        # Update the Q-value  
#        Q[current_state[0], current_state[1], actions.index(action)] = Q[current_state[0], current_state[1], actions.index(action)] + alpha * (reward + gamma * np.max(Q[next_state[0], next_state[1]]) - Q[current_state[0], current_state[1], actions.index(action)])
        
        # Update the state
        current_state = next_state
#        if reward >= 100:
#            break
        sts.append(current_state)
            
    ################### STL Rewrad ##################
    x_tr = []
    y_tr = []
    for i in range(len(sts)):
        x_tr.append([i,sts[i][0]])
        y_tr.append([i,sts[i][1]])
        
    dataset = (x_tr,y_tr)
    rob = max(robustness(dataset, spec))
    rob_goal = max(robustness(dataset, spec_goal))
    rob_obs = max(robustness(dataset, spec_obs))
   
    if rob_obs >=0:
        reward = rob -100
        next_state = current_state
    elif rob_goal >=0:
        reward = rob + 100
    else:
        reward = rob
#    print(sts)
#    print(reward)
    # Update the Q-values at the end of the episode using the cumulative episode reward
    Q[current_state[0], current_state[1], actions.index(action)] = Q[current_state[0], current_state[1], actions.index(action)] + alpha * (reward - Q[current_state[0], current_state[1], actions.index(action)])


        
# Generate 500 rollout traces
rollout_traces = []
actionnn = []
for i in range(20):
    current_state = (0, 0)
    rollout = [current_state]
    rob_goal = -1
    act = []
    for k in range(20):
#    while rob_goal < 0:
        # print(current_state)
        print(current_state)
        action = actions[np.argmax(Q[current_state[0], current_state[1]])]
        act.append(action)
        next_state = state_transition(current_state,action)
#
#        rob_obs = robustness(spec_obs, next_state)
#        rob_goal = robustness(spec_goal, next_state)
        
#        if rob_obs >=0:
#            next_state = current_state
            
        current_state = next_state
        rollout.append(current_state)
    actionnn.append(act)
    rollout_traces.append(rollout)
    
# Organize rollout traces into a dataset
x_trace = []
y_trace = []
for i in range(len(rollout_traces)):
    x_vals = []
    y_vals = []
    for j in rollout_traces[i]:
        x_vals.append(j[0]+0.5)
        y_vals.append(j[1]+0.5)
    x_trace.append(x_vals)
    y_trace.append(y_vals)
# Display the grid environment with the rollout traces in black, goal position in green, and obstacle in red



# fig = plt.figure()
for i in range(len(rollout_traces)):
    fig = plt.figure()
    goal = matplotlib.patches.Rectangle((4,4), 2, 2, color='green')
    obstacle1 = matplotlib.patches.Rectangle((2,2), 3, 1, color='black')
    obstacle2 = matplotlib.patches.Rectangle((4,3), 1, 1, color='black')
    ax = fig.add_subplot(111)
    ax.add_patch(goal)
    ax.add_patch(obstacle1)
    ax.add_patch(obstacle2)
    plt.plot(x_trace[i],y_trace[i],'*-')
    plt.plot(x_trace[i],y_trace[i],'c*-')
    plt.grid()
    plt.show()
    print(actionnn[i])
#plt.plot(x_trace[i],y_trace[i],'*-')
#plt.grid()    
#plt.show()


