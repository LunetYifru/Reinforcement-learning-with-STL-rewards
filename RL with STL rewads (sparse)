#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Mar 19 15:03:00 2023

@author: lay0005
"""

import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
import matplotlib
import rtamt

grid_size = 6

def specification(spec_str):
    spec = rtamt.StlDenseTimeSpecification()
    spec.name = 'STL dense-time online monitor'
    spec.declare_var('x', 'float')
    spec.declare_var('y', 'float')
    spec.spec = spec_str
    spec.parse()
    return spec

def robustness(dataset, spec):
    rob = spec.evaluate(['x',dataset[0]],['y',dataset[1]])
    return [i[1] for i in rob]

def robustness_cs(spec_str,current_state):  # Online Monitor
    spec = rtamt.StlDiscreteTimeSpecification()
    spec.declare_var('x', 'float')
    spec.declare_var('y', 'float')
    spec.spec = spec_str
    spec.parse()
    rob = spec.update(0, [('x', current_state[0]) , ('y', current_state[1])])
    return rob

# stl initialize
spec_goal = specification('F(x>=4 & y>=4)') #specification representing the goal location
spec_obs = specification('F((x>=2 & x < 5 & y >= 2 & y < 3) or (x > 3 & x <= 4 & y > 2 & y <= 3))') #specification representing the obstacle location
spec = specification('G(F(x>=4 & y>=4) and (not(F((x>=2 & x < 5 & y >= 2 & y < 3) or (x > 3 & x <= 4 & y > 2 & y <= 3)))))')
    
def state_transition(current_state, action):
    if action == (0,-1):#down
        suggested_state = (current_state[0] + action[0], max(0,current_state[1] + action[1]))
    elif action == (0,1):#up
        suggested_state = (current_state[0] + action[0], min(grid_size-1,current_state[1] + action[1]))
    elif action ==(-1,0):#left
        suggested_state = (max(0,current_state[0] + action[0]), current_state[1] + action[1])
    elif action == (1,0):#right
        suggested_state = (min(grid_size-1,current_state[0] + action[0]), current_state[1] + action[1])
    
#    
    # up, down,right, left
    states = [(current_state[0] + 0, min(grid_size-1,current_state[1] + 1)), (current_state[0] + 0, max(0,current_state[1]  - 1)), ((min(grid_size-1,current_state[0] + 1), current_state[1] + 0)) ,(max(0,current_state[0] -1), current_state[1] + 0)]
   
    
    num_states = len(states)
    transition_probabilities = [0.9 if state == suggested_state else (0.1/(num_states-1)) for state in states]
    next_state = random.choices(states,weights=transition_probabilities) 
    return next_state[0]


# Define the actions
actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # up, down,right, left

# Initialize Q-table
Q = np.zeros((grid_size, grid_size, len(actions)))

# Define the training parameters
alpha = 0.5
gamma = 0.8
epsilon = 0.9
max_steps = 30
num_episodes = 10000
decay_rate = -0.0002
er = []
# Train the agent
for episode in range(num_episodes):
    print(episode)
    # Initialize the state
    current_state = (0,0)
    epsilon = np.exp(-0.005*episode)
    states_actions = []
    # Terminate the episode if the agent reaches the goal or the maximum number of steps is reached
    for step in range(max_steps):
        # Choose an action using epsilon-greedy exploration
        if random.uniform(0, 1) < epsilon: #explore
            action = random.choice(actions)
        else:
            action = actions[np.argmax(Q[current_state[0], current_state[1]])] #exploit
                              
        # Determine the next state following the transition probablities        
        next_state = state_transition(current_state,action)
        
        # Store the current state and action
        states_actions.append((current_state, action))
        
        # Update the state
        current_state = next_state
            
    ################### STL Rewrad ##################
    x_tr = []
    y_tr = []
    for i in range(len(states_actions)):
        x_tr.append([i,states_actions[i][0][0]])
        y_tr.append([i,states_actions[i][0][1]])
        
    dataset = (x_tr,y_tr)
    rob = robustness(dataset, spec)[0]
    rob_goal = robustness(dataset, spec_goal)[0]
    rob_obs = robustness(dataset, spec_obs)[0]
   
    if rob_obs >=0 and rob_goal >=0:
        reward = rob
    elif rob_obs <0 and rob_goal >=0:
        reward = rob + 100
    elif rob_obs >=0 and rob_goal <0:
        reward = rob -100
    elif rob_obs <0 and rob_goal <0:
        reward = rob -50
        
    print(rob_goal, rob_obs, rob, reward)
#    print(sts)
#    print(reward)
    
    # Update the Q-values at the end of the episode using the cumulative episode reward
    for i, (state, action) in enumerate(states_actions):
#       Q[state[0], state[1], actions.index(action)] = Q[state[0], state[1], actions.index(action)] + alpha * (reward - Q[state[0], state[1], actions.index(action)])
        Q[state[0], state[1], actions.index(action)] += alpha * (reward + gamma * np.max(Q[states_actions[i+1 if i<len(states_actions)-1 else i][0][0], states_actions[i+1 if i<len(states_actions)-1 else i][0][1], :]) - Q[state[0], state[1], actions.index(action)])


        
# Generate 500 rollout traces
rollout_traces = []
actionnn = []
for i in range(20):
    current_state = (0, 0)
    rollout = [current_state]
    rob_goal = -1
    act = []
    st = 0
    #for k in range(30):
    while rob_goal < 0 and st <30:
        # print(current_state)
        print(current_state)
        action = actions[np.argmax(Q[current_state[0], current_state[1]])]
        act.append(action)
        next_state = state_transition(current_state,action)
#
#        rob_obs = robustness(spec_obs, next_state)
        rob_goal = robustness_cs('x>=4 & y>=4', next_state)
        
#        if rob_obs >=0:
#            next_state = current_state
            
        current_state = next_state
        rollout.append(current_state)
        st+=1
    actionnn.append(act)
    rollout_traces.append(rollout)
    
# Organize rollout traces into a dataset
x_trace = []
y_trace = []
for i in range(len(rollout_traces)):
    x_vals = []
    y_vals = []
    for j in rollout_traces[i]:
        x_vals.append(j[0])
        y_vals.append(j[1])
    x_trace.append(x_vals)
    y_trace.append(y_vals)
# Display the grid environment with the rollout traces in black, goal position in green, and obstacle in red
x_trace = pd.DataFrame(x_trace).T
y_trace = pd.DataFrame(y_trace).T

x_plt = []
y_plt = []
# fig = plt.figure()
for i in range(len(x_trace.columns)):
    x_plt.append([j+0.5 for j in x_trace[i]])
    y_plt.append([j+0.5 for j in y_trace[i]])

for i in range(len(rollout_traces)):
    fig = plt.figure()
    goal = matplotlib.patches.Rectangle((4,4), 2, 2, color='green')
    obstacle1 = matplotlib.patches.Rectangle((2,2), 3, 1, color='black')
    obstacle2 = matplotlib.patches.Rectangle((4,3), 1, 1, color='black')
    ax = fig.add_subplot(111)
    ax.add_patch(goal)
    ax.add_patch(obstacle1)
    ax.add_patch(obstacle2)
#    plt.plot(x_plt[i],y_plt[i],'*-')
    plt.plot(x_plt[i],y_plt[i],'c*-')
    plt.grid()
    plt.show()
#plt.plot(x_trace[i],y_trace[i],'*-')
#plt.grid()    
#plt.show()
